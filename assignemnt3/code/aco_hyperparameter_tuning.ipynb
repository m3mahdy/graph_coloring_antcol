{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca71261",
   "metadata": {},
   "source": [
    "# ACO Hyperparameter Tuning for Graph Coloring Problem\n",
    "\n",
    "This notebook performs hyperparameter tuning for Ant Colony Optimization (ACO) algorithm applied to the Graph Coloring Problem using Optuna."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a0f0d5",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ccd402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Check if running in Google Colab environment\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(f\"Running in Google Colab: {IS_COLAB}\")\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"Colab environment detected. Will mount Google Drive.\")\n",
    "    # Mount Google Drive if running in Colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully at /content/drive\")\n",
    "else:\n",
    "    print(\"Local environment detected. Using local paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628428d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths for data, studies, results, and figures based on the execution environment.\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure base paths based on environment\n",
    "if IS_COLAB:\n",
    "    # Update this path to match your Google Drive structure\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/meta_graph_coloring_antcol/assignemnt3')\n",
    "    CODE_PATH = BASE_PATH / 'code'\n",
    "    # Add code path to system path for imports\n",
    "    sys.path.insert(0, str(CODE_PATH))\n",
    "else:\n",
    "    # Local environment paths\n",
    "    BASE_PATH = Path('/Users/mahdy/projects/meta_graph_coloring_antcol/assignemnt3')\n",
    "    CODE_PATH = BASE_PATH / 'code'\n",
    "\n",
    "# Define data root path (contains tiny_dataset and main_dataset)\n",
    "DATA_ROOT = BASE_PATH / 'data'\n",
    "\n",
    "# Verify paths exist\n",
    "if not BASE_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Base path does not exist: {BASE_PATH}\")\n",
    "if not DATA_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Data root does not exist: {DATA_ROOT}\")\n",
    "\n",
    "print(f\"Base Path: {BASE_PATH}\")\n",
    "print(f\"Code Path: {CODE_PATH}\")\n",
    "print(f\"Data Root: {DATA_ROOT}\")\n",
    "print(f\"\\nPath verification: OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a497653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if running in Colab\n",
    "if IS_COLAB:\n",
    "    print(\"Installing required packages...\")\n",
    "    !pip install -q networkx==3.2.1 matplotlib==3.8.2 pandas==2.1.4 numpy==1.26.2 optuna==3.5.0 scikit-learn==1.4.0 scipy==1.11.4\n",
    "    print(\"Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98d30c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "from IPython.display import Image, display, Markdown, HTML\n",
    "\n",
    "# Import project modules\n",
    "from dataloader import GraphDataLoader\n",
    "from optuna_tuner import OptunaACOTuner\n",
    "from aco_gpc import ACOGraphColoring\n",
    "from objective_function import aco_objective_function\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540879de",
   "metadata": {},
   "source": [
    "## 2. Study Configuration\n",
    "\n",
    "Define dataset selection, study name, and hyperparameter search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e57d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset selection: 'tiny_dataset' for quick testing, 'main_dataset' for full experiments\n",
    "DATASET_NAME = 'main_dataset'  # Change to 'main_dataset' for full tuning\n",
    "\n",
    "# Study continuation mode\n",
    "# Option 1: Continue existing study - set CONTINUE_STUDY to the study name\n",
    "# Example: CONTINUE_STUDY = 'aco_study_tiny_dataset_20251129_172927'\n",
    "# Option 2: Start new study - set CONTINUE_STUDY = None\n",
    "CONTINUE_STUDY = None  # Set to study name to continue, or None for new study\n",
    "\n",
    "# Study name (used for new studies or when CONTINUE_STUDY is None)\n",
    "if CONTINUE_STUDY:\n",
    "    STUDY_NAME = CONTINUE_STUDY\n",
    "    print(f\"Continuing existing study: {STUDY_NAME}\")\n",
    "else:\n",
    "    STUDY_NAME = f'aco_study_{DATASET_NAME}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "    print(f\"Creating new study: {STUDY_NAME}\")\n",
    "\n",
    "# Number of Optuna trials for hyperparameter tuning\n",
    "N_TRIALS = 100\n",
    "\n",
    "# Number of random exploration trials before optimization starts\n",
    "N_STARTUP_TRIALS = 5\n",
    "\n",
    "# ACO verbose setting \n",
    "ACO_VERBOSE = False   # Set to True to see detailed ACO progress\n",
    "\n",
    "# Hyperparameter search space configuration (parameters to optimize)\n",
    "PARAM_CONFIG = {\n",
    "    'iterations': {\n",
    "        'type': 'int',\n",
    "        'low': 10,\n",
    "        'high': 200,\n",
    "    },\n",
    "    'alpha': {\n",
    "        'type': 'float',\n",
    "        'low': 0.5,\n",
    "        'high': 3.0,\n",
    "    },\n",
    "    'beta': {\n",
    "        'type': 'float',\n",
    "        'low': 1.0,\n",
    "        'high': 6.0,\n",
    "    },\n",
    "    'rho': {\n",
    "        'type': 'float',\n",
    "        'low': 0.001,\n",
    "        'high': 0.5,\n",
    "    },\n",
    "    'ant_count': {\n",
    "        'type': 'int',\n",
    "        'low': 10,\n",
    "        'high': 50,\n",
    "    },\n",
    "    'Q': {\n",
    "        'type': 'float',\n",
    "        'low': 0.1,\n",
    "        'high': 10.0,\n",
    "    },\n",
    "    'patience': {\n",
    "        'type': 'float',\n",
    "        'low': 0.1,\n",
    "        'high': 0.3,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Dataset: {DATASET_NAME}\")\n",
    "print(f\"  Study Name: {STUDY_NAME}\")\n",
    "print(f\"  Optuna Trials: {N_TRIALS}\")\n",
    "print(f\"  Random Exploration Trials: {N_STARTUP_TRIALS} (before optimization)\")\n",
    "print(f\"\\nHyperparameters to Optimize:\")\n",
    "for param_name, param_spec in PARAM_CONFIG.items():\n",
    "    print(f\"  {param_name}: [{param_spec['low']}, {param_spec['high']}] ({param_spec['type']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d95810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Optuna tuner\n",
    "tuner = OptunaACOTuner(\n",
    "    study_name=STUDY_NAME,\n",
    "    data_root=str(DATA_ROOT),\n",
    "    direction='minimize'  # We want to minimize the number of colors used\n",
    ")\n",
    "\n",
    "print(f\"Tuner initialized with study: {STUDY_NAME}\")\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "\n",
    "# Load tuning dataset once (before optimization)\n",
    "print(\"\\nLoading tuning dataset...\")\n",
    "data_loader = GraphDataLoader(str(DATA_ROOT), DATASET_NAME)\n",
    "tuning_graphs = data_loader.load_tuning_dataset()\n",
    "print(f\"Loaded {len(tuning_graphs)} graphs for tuning\\n\")\n",
    "\n",
    "# Wrapper function to pass tuning graphs to objective function\n",
    "def objective_wrapper(trial, params, **kwargs):\n",
    "    return aco_objective_function(\n",
    "        trial=trial,\n",
    "        params=params,\n",
    "        tuning_graphs=tuning_graphs,\n",
    "        aco_class=ACOGraphColoring,\n",
    "        verbose=ACO_VERBOSE\n",
    "    )\n",
    "\n",
    "print(\"Objective function wrapper ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d076f0",
   "metadata": {},
   "source": [
    "## 3. Run Hyperparmeters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4bbc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the hyperparameter optimization\n",
    "print(f\"\\nStarting hyperparameter optimization with {N_TRIALS} trials...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_params = tuner.optimize(\n",
    "    objective_func=objective_wrapper,\n",
    "    param_config=PARAM_CONFIG,\n",
    "    aco_class=ACOGraphColoring,\n",
    "    n_trials=N_TRIALS\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Optimization completed!\")\n",
    "print(\"\\nBest Parameters Found:\")\n",
    "for param_name, param_value in best_params.items():\n",
    "    print(f\"  {param_name}: {param_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8c6ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all optimization plots\n",
    "print(\"\\nGenerating optimization visualization plots...\")\n",
    "print(\"=\" * 70)\n",
    "tuner.generate_plots(recreate=True)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5783b349",
   "metadata": {},
   "source": [
    "### Best Trial Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482539f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best trial graphs and metrics\n",
    "\n",
    "\n",
    "# Get best trial number\n",
    "best_trial_number = tuner.study.best_trial.number\n",
    "best_trial_dir = Path(DATA_ROOT) / 'studies' / STUDY_NAME / 'results' / f'trial_{best_trial_number:04d}'\n",
    "\n",
    "print(f\"Best Trial: {best_trial_number}\")\n",
    "print(f\"Best Objective Value: {tuner.study.best_value}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Display metrics for best trial\n",
    "display(Markdown(f\"#### Best Trial Metrics\"))\n",
    "\n",
    "metric_files = [\n",
    "    ('color_count.png', 'Color Count per Graph'),\n",
    "    ('execution_time.png', 'Execution Time per Graph'),\n",
    "    ('conflicts.png', 'Conflicts per Graph')\n",
    "]\n",
    "\n",
    "for filename, title in metric_files:\n",
    "    metric_path = best_trial_dir / filename\n",
    "    if metric_path.exists():\n",
    "        display(Markdown(f\"**{title}**\"))\n",
    "        display(HTML(f'<img src=\"{metric_path}\" style=\"width: 75%; display: block; margin: 0 auto;\">'))\n",
    "\n",
    "# Display colored graphs for best trial\n",
    "display(Markdown(f\"#### Best Trial: Colored Graph Solutions\"))\n",
    "\n",
    "# Find all graph files in best trial directory\n",
    "graph_files = sorted(best_trial_dir.glob('graph_*.png'))\n",
    "for graph_file in graph_files:\n",
    "    graph_name = graph_file.stem.replace('graph_', '')\n",
    "    display(Markdown(f\"**{graph_name}**\"))\n",
    "    display(HTML(f'<img src=\"{graph_file}\" style=\"width: 75%; display: block; margin: 0 auto;\">'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9404e951",
   "metadata": {},
   "source": [
    "### Display Study Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b380926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all study figures\n",
    "\n",
    "study_figures_path = Path(DATA_ROOT) / 'studies' / STUDY_NAME / 'figures'\n",
    "\n",
    "# List of all possible study figures with descriptions\n",
    "figure_files = [\n",
    "    ('history.png', 'Optimization History', 'Shows objective value progression across trials'),\n",
    "    ('importances.png', 'Parameter Importances', 'Shows which hyperparameters impact results most'),\n",
    "    ('slice.png', 'Slice Plots (All Parameters)', 'Shows how each parameter affects objective value'),\n",
    "    ('timeline.png', 'Trial Timeline', 'Shows when each trial ran and how long it took')\n",
    "]\n",
    "\n",
    "print(\"Study Visualization Figures:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for filename, title, description in figure_files:\n",
    "    figure_path = study_figures_path / filename\n",
    "    if figure_path.exists():\n",
    "        display(Markdown(f\"### {title}\"))\n",
    "        display(Markdown(f\"*{description}*\"))\n",
    "        display(HTML(f'<img src=\"{figure_path}\" style=\"width: 75%; display: block; margin: 0 auto;\">'))\n",
    "        print(f\"‚úì {filename}\")\n",
    "    else:\n",
    "        print(f\"‚úó {filename} (not generated - may require additional dependencies)\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63621ea5",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Algorithms Comparison\n",
    "\n",
    "Compare three algorithms on the testing dataset:\n",
    "- **Greedy Algorithm**: Fast constructive heuristic\n",
    "- **Tabu Search**: Single-solution metaheuristic with memory\n",
    "- **ACO**: Population-based metaheuristic (using best parameters from tuning)\n",
    "\n",
    "Each algorithm runs 5 times on all test graphs to calculate statistics and generate comparison plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e8da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the comprehensive testing module\n",
    "from comprehensive_testing import run_comprehensive_testing\n",
    "\n",
    "# Set up testing directories - save comparison inside study path\n",
    "comparison_output_dir = DATA_ROOT / 'studies' / STUDY_NAME / 'algorithm_comparison'\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Comparison output directory: {comparison_output_dir}\")\n",
    "print(f\"  (Results will be saved in study: {STUDY_NAME})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1560ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare ACO parameters from best trial\n",
    "aco_test_params = {\n",
    "    'iterations': int(best_params['iterations']),\n",
    "    'alpha': best_params['alpha'],\n",
    "    'beta': best_params['beta'],\n",
    "    'rho': best_params['rho'],\n",
    "    'ant_count': int(best_params['ant_count']),\n",
    "    'Q': best_params['Q'],\n",
    "    'patience': best_params['patience']\n",
    "}\n",
    "\n",
    "print(\"\\nACO Parameters for Testing (from best trial):\")\n",
    "for param, value in aco_test_params.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3566d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive testing: 5 repetitions per algorithm per graph\n",
    "# Results are cached in JSON - set force_rerun=True to re-execute tests\n",
    "NUM_REPETITIONS = 5\n",
    "FORCE_RERUN = False  # Set to True to ignore cached results and rerun testing\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Running comprehensive testing with {NUM_REPETITIONS} repetitions per algorithm\")\n",
    "if FORCE_RERUN:\n",
    "    print(\"‚ö† FORCE_RERUN=True: Will ignore cached results and rerun all tests\")\n",
    "else:\n",
    "    print(\"‚Ñπ Using cached results if available (set FORCE_RERUN=True to rerun)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "df_comparison_results, df_comparison_stats = run_comprehensive_testing(\n",
    "    data_root=str(DATA_ROOT),\n",
    "    dataset_name=DATASET_NAME,\n",
    "    output_dir=str(comparison_output_dir),\n",
    "    num_repetitions=NUM_REPETITIONS,\n",
    "    aco_params=aco_test_params,\n",
    "    force_rerun=FORCE_RERUN\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Comprehensive testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4f9fa5",
   "metadata": {},
   "source": [
    "### 6.1 Display Statistics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f5e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistics table\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "print(\"Statistics Summary:\")\n",
    "display(df_comparison_stats)\n",
    "\n",
    "# Print formatted statistics (without markdown)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(df_comparison_stats.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb05492",
   "metadata": {},
   "source": [
    "### 6.2 Display Comparison Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9c7e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display algorithm comparison plots (4 comparison metrics)\n",
    "from IPython.display import Image\n",
    "\n",
    "# Define all comparison plots\n",
    "comparison_plots = [\n",
    "    ('comparison_best_colors.png', 'Best Color Count Comparison (with Best Known Solutions)'),\n",
    "    ('comparison_avg_colors.png', 'Average Color Count Comparison (with Std Dev)'),\n",
    "    ('comparison_execution_time.png', 'Execution Time Comparison (log scale)'),\n",
    "    ('comparison_conflicts.png', 'Conflict Count Comparison')\n",
    "]\n",
    "\n",
    "print(\"Algorithm Comparison Metrics:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for filename, title in comparison_plots:\n",
    "    plot_path = comparison_output_dir / filename\n",
    "    if plot_path.exists():\n",
    "        display(Markdown(f\"### {title}\"))\n",
    "        display(Markdown(f'<img src=\"{plot_path}\" style=\"width: 75%; display: block; margin: 0 auto;\"/>'))\n",
    "    else:\n",
    "        print(f\"‚ö† {title} not found: {filename}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b71ce4",
   "metadata": {},
   "source": [
    "### 6.3 Results Summary and File Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6667a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive testing results summary\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE TESTING RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìÅ Output Directory: {comparison_output_dir}\")\n",
    "print(f\"   (Saved in study: {STUDY_NAME})\")\n",
    "\n",
    "print(f\"\\nüìä Generated Files:\")\n",
    "print(f\"   - comparison_results.json (complete results - cached for reuse)\")\n",
    "print(f\"   - comprehensive_test_results.csv (all {len(df_comparison_results)} individual runs)\")\n",
    "print(f\"   - statistics_summary.csv (aggregated statistics)\")\n",
    "print(f\"   - comparison_best_colors.png (best color count comparison)\")\n",
    "print(f\"   - comparison_avg_colors.png (average color count with std dev)\")\n",
    "print(f\"   - comparison_execution_time.png (execution time comparison)\")\n",
    "print(f\"   - comparison_conflicts.png (conflict count comparison)\")\n",
    "print(f\"   - color_distribution_boxplots.png (distribution analysis)\")\n",
    "\n",
    "print(f\"\\nüí° Tip: Results are cached in JSON. Re-running this cell will use cached data.\")\n",
    "print(f\"   Set FORCE_RERUN=True to ignore cache and rerun all tests.\")\n",
    "\n",
    "print(f\"\\nüéØ Summary by Algorithm:\")\n",
    "for algo_name in df_comparison_stats['algorithm'].unique():\n",
    "    df_algo = df_comparison_stats[df_comparison_stats['algorithm'] == algo_name]\n",
    "    best_colors = df_algo['best_colors'].min()\n",
    "    avg_colors = df_algo['avg_colors'].mean()\n",
    "    avg_time = df_algo['avg_time'].mean()\n",
    "    print(f\"   {algo_name}:\")\n",
    "    print(f\"      Best colors: {best_colors}\")\n",
    "    print(f\"      Avg colors: {avg_colors:.2f}\")\n",
    "    print(f\"      Avg time: {avg_time:.4f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
